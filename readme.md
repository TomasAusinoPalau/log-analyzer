# Log Analyzer

Log Analyzer is a command-line tool designed to process and analyze web server log files. It identifies user sessions, calculates various metrics, and generates a detailed report.

---

## Features

- **User Session Analysis**: Detects user sessions based on a 10-minute inactivity threshold.
- **Top User Insights**: Displays top users by page views with session details.
- **Parallel and non-blocking processing**: File reading and report generation are async and non blocking, providing efficiency and scalability.
---

## Table of Contents

1. [Getting Started](#getting-started)
2. [Usage](#usage)
3. [Output Example](#output-example)
4. [Core Architecture](#core-architecture)
5. [Priorities in the Design Process](#priorities-in-the-design-process)

---

## Getting Started

Follow these instructions to set up and use the Log Analyzer on your local machine.

### Prerequisites

- **Scala 2.13**
- **SBT (Scala Build Tool)**
- **Java 8 or newer**

### Installation
Non installation required, follow this steps:

1. Unzip the compressed folder.
2. Access the log-analyzer directory.
```
cd log-analyzer 
```

## Usage

1. Compile the project:
```
sbt compile
```

2. Run the analyzer with the path to your log files directory:
```
sbt "run /path/to/logs/directory"
```

## Output Example

An example report generated by the Log Analyzer:

Total unique users: 27  
Top users:

| id       | # pages | # sess | longest | shortest |
|----------|---------|--------|---------|----------|
| 71f28176 | 75      | 3      | 35      | 1        |
| 41f58122 | 65      | 4      | 60      | 10       |
| 58122233 | 44      | 2      | 121     | 3        |
| 43122543 | 40      | 4      | 50      | 4        |
| 52123456 | 33      | 3      | 100     | 1        |

## Core Architecture

This section outlines the key components of the project, focusing on their responsibilities and how they interact to achieve a modular and scalable design. Each component is designed with reusability and clarity in mind to ensure maintainable and efficient log processing.

### Components

- **LogAnalyzer**:  
  The main class of the project, responsible for orchestrating the overall workflow.

- **LogAnalyzerHelper**:  
  A trait that encapsulates core functionalities, such as reading log files, transforming logs into metrics, and printing user reports. It ensures modularity and reusability.

- **LogEntryHelper**:  
  A trait providing utility methods for parsing raw log lines into structured `LogEntry` objects. It acts as a reusable component for handling log line parsing logic.

- **ScriptExecutionContext**:  
  Provides a custom thread pool for handling asynchronous operations efficiently, avoiding overloading the global execution context.
## Priorities in the Design Process

1. **Simplicity and Clarity**: The codebase was structured to prioritize readability and maintainability, ensuring that it is easy to understand and extend the functionality.


2. **Asynchronous Processing**: Given the nature of log processing and potentially large data volumes, we designed the system to handle tasks asynchronously. 
This approach optimizes performance and ensures scalability for larger datasets without blocking the main thread.


3. **Modular Architecture**: Each component was designed with single responsibility principles, allowing individual modules to be tested and reused independently.


4. **Functional Paradigms**: The implementation leverages functional programming principles, such as immutability and higher-order functions, to ensure predictable and reliable behavior.


5. **Robust log parsing**: Regular expression ensures that only logs matching the expected format are processed, reducing the risk of invalid data propagation.

## FAQ (Frequently Asked Questions)

### 1. **What is the purpose of this project?**
This project is designed to process and analyze log files, extracting key metrics such as user activity, session durations, and usage patterns. The system is modular, scalable, and built for efficient log processing with asynchronous operations.

---

### 2. **How does the project handle malformed or invalid log entries?**
Malformed log entries are filtered out during the parsing process using the `parseLogLine` method. This method ensures only valid logs are processed by returning `None` for entries that do not match the expected format or contain invalid data.

---

### 3. **What is the role of `ScriptExecutionContext`?**
`ScriptExecutionContext` provides a custom thread pool for managing asynchronous operations in the project. It ensures that the global execution context is not overloaded, optimizing the performance of tasks like file reading and metrics generation.

---

### 4. **How are logs grouped into sessions?**
Logs are grouped into sessions using the `groupLogsIntoSessions` method. A session is defined as a group of logs where the time difference between consecutive entries is 10 minutes or less. If the time difference exceeds 10 minutes, a new session is created.

---

### 5. **Can the system handle large volumes of log files?**
Yes, the project is designed to handle large volumes of logs efficiently. It uses asynchronous processing (`Future`) and scalable components like `readLogsFromDirectory` and `logsToMetrics` to ensure performance and reliability when dealing with large datasets.

